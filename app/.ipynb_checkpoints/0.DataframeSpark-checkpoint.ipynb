{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uso do Spark data Sources\n",
    "\n",
    "O **Spark Data Sources** é uma ferramenta poderosa no ecossistema Spark para carregar e salvar dados de várias fontes. Vamos dar uma olhada em como usá-lo!\n",
    "\n",
    "Abaixo os principais tipos de sources:\n",
    "\n",
    "| Tipo de Fonte de Dados | Descrição                                                                                     |\n",
    "|------------------------|-----------------------------------------------------------------------------------------------|\n",
    "| Parquet                | Formato de arquivo colunar eficiente e otimizado para análise que é amplamente suportado.    |\n",
    "| ORC                    | Formato de arquivo colunar otimizado para consulta que é amplamente usado em ecossistemas do Hadoop. |\n",
    "| JSON                   | Um formato de arquivo de dados textual amplamente utilizado.                                 |\n",
    "| CSV                    | Formato de arquivo de texto separado por vírgulas.                                            |\n",
    "| Avro                   | Estrutura de dados em formato binário compacto.                                                |\n",
    "| Delta Lake             | Uma camada de armazenamento de dados do tipo \"lakehouse\" que oferece controle transacional, governança e otimizações. |\n",
    "| Hadoop File System (HDFS) | Sistema de arquivos distribuído amplamente utilizado para armazenamento de dados.         |\n",
    "| Cassandra              | Banco de dados distribuído NoSQL amplamente utilizado.                                       |\n",
    "| Elasticsearch          | Motor de busca e análise em tempo real.                                                      |\n",
    "| Kafka                  | Plataforma de streaming distribuída para processamento de eventos em tempo real.             |\n",
    "| JDBC                   | Acesso a bancos de dados relacionais usando o Java Database Connectivity (JDBC).           |\n",
    "| Table                 | Consulta de tabelas usando SQL.                                                                |\n",
    "\n",
    "---\n",
    "## DataFrames no Apache Spark\n",
    "\n",
    "O Apache Spark é uma ferramenta poderosa para processamento de dados em larga escala e os DataFrames são uma parte fundamental dessa tecnologia. Um DataFrame é uma abstração de dados distribuídos que fornece uma interface fácil de usar para manipular dados em lote e em tempo real.\n",
    "\n",
    "## Características Principais\n",
    "\n",
    "Os DataFrames no Spark têm várias características essenciais:\n",
    "\n",
    "- **Estrutura de Dados**: Um DataFrame é uma estrutura de dados tabular que se assemelha a uma tabela em um banco de dados relacional. Ele tem linhas e colunas, permitindo que você organize e manipule dados de maneira eficiente.\n",
    "\n",
    "- **Tipagem de Colunas**: Cada coluna em um DataFrame possui um tipo de dados associado, permitindo que o Spark otimize o processamento e a execução de consultas.\n",
    "\n",
    "- **Transformações e Ações**: Você pode realizar transformações complexas nos DataFrames, como filtragem, agregação e junção de dados. Além disso, você pode executar ações para coletar resultados ou gravar dados em fontes externas.\n",
    "\n",
    "- **Integração com Diversas Fontes de Dados**: Os DataFrames no Spark podem ser usados com várias fontes de dados, como arquivos CSV, JSON, Parquet, bancos de dados relacionais, sistemas de mensagens, entre outros.\n",
    "\n",
    "- **Suporte a SQL**: Você pode executar consultas SQL em DataFrames, tornando-o uma ferramenta poderosa para análise de dados.\n",
    "\n",
    "## Carregando e Salvando Dados como dataframes\n",
    "\n",
    "Documentação oficial:\n",
    "- [Spark SQL Data Sources](https://spark.apache.org/docs/3.3.2/sql-data-sources.html)\n",
    "- [Spark DataFrames](https://spark.apache.org/docs/3.3.2/api/python/reference/pyspark.sql/dataframe.html)\n",
    "\n",
    "Aqui, vamos mostrar um exemplo de como carregar e salvar dados usando o Spark SQL Data Sources.\n",
    "\n",
    "### Carregando Dados\n",
    "\n",
    "Para carregar dados, você pode usar o método `read` do objeto `SparkSession`. Veja como:\n",
    "\n",
    "```python\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Exemplo\").getOrCreate()\n",
    "\n",
    "# Carregando dados de um arquivo CSV\n",
    "df = spark.read.csv(\"caminho/para/seu/arquivo.csv\")\n",
    "```\n",
    "\n",
    "A tabela abaixo detalha algumas opções comuns que você pode passar para o método read:\n",
    "\n",
    "| Opção            | Descrição                                    |\n",
    "|------------------|----------------------------------------------|\n",
    "| `format`         | Formato do arquivo (por ex., \"csv\", \"parquet\") |\n",
    "| `option`         | Opções específicas do formato                |\n",
    "| `header`         | Se a primeira linha contém nomes de colunas |\n",
    "| `inferSchema`    | Inferir automaticamente o esquema dos dados  |\n",
    "\n",
    "\n",
    "### Salvando Dados\n",
    "\n",
    "Agora, vamos ver como salvar dados usando o método `write`:\n",
    "\n",
    "```python\n",
    "# Salvando o DataFrame em formato Parquet\n",
    "df.write.parquet(\"caminho/para/seu/arquivo.parquet\")\n",
    "```\n",
    "\n",
    "Aqui estão algumas opções que você pode usar ao salvar:\n",
    "\n",
    "| Opção            | Descrição                                    |\n",
    "|------------------|----------------------------------------------|\n",
    "| `format`         | Formato de saída (por ex., \"parquet\", \"csv\") |\n",
    "| `mode`           | Modo de escrita (\"append\", \"overwrite\", etc.) |\n",
    "| `partitionBy`    | Colunas para particionar os dados           |\n",
    "| `option`         | Opções específicas do formato                |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Entendendo a leitura de dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    " from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Exemplo\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Carregando dados de um arquivo CSV\n",
    "df = (\n",
    "    spark\n",
    "    .read\n",
    "    .csv(\"data/landing/VRA/\")\n",
    ")\n",
    "df.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df2 = (\n",
    "    spark\n",
    "    .read\n",
    "    .format(\"csv\")\n",
    "    .option(\"inferSchema\",\"true\")\n",
    "    .option(\"encoding\",\"utf-8\")\n",
    "    .option(\"delimiter\",\";\") #ou sep\n",
    "    .load(\"data/landing/VRA/\")\n",
    ")\n",
    "\n",
    "df2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df3 = (\n",
    "    spark\n",
    "    .read\n",
    "    .format(\"csv\")\n",
    "    .option(\"inferSchema\",\"true\")\n",
    "    .option(\"encoding\",\"utf-8\")\n",
    "    .option(\"delimiter\",\";\") #ou sep\n",
    "    .option(\"header\", \"true\")\n",
    "    .load(\"data/landing/VRA/\")\n",
    ")\n",
    "\n",
    "df3.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Entendendo os schemas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "schema_json = df3.schema.json()\n",
    "ddl = spark.sparkContext._jvm.org.apache.spark.sql.types.DataType.fromJson(schema_json).toDDL()\n",
    "\n",
    "ddl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "schema_ddl = \"\"\"\n",
    "    `ICAO Empresa Aerea` STRING,\n",
    "    `Numero Voo` STRING,\n",
    "    `Codigo DI` STRING,\n",
    "    `Codigo Tipo Linha` STRING,\n",
    "    `ICAO Aerodromo Origem` STRING,\n",
    "    `ICAO Aerodromo Destino` STRING,\n",
    "    `Partida Prevista` timestamp,\n",
    "    `Partida Real` timestamp,\n",
    "    `Chegada Prevista` timestamp,\n",
    "    `Chegada Real` timestamp,\n",
    "    `Situacao Voo` STRING,\n",
    "    `Codigo Justificativa` STRING\n",
    "\"\"\"\n",
    "\n",
    "schema_method = StructType(\n",
    "    [\n",
    "        StructField('chegada_prevista', IntegerType()),\n",
    "        StructField('chegada_real', IntegerType()),\n",
    "        StructField('codigo_autorizacao', StringType()),\n",
    "        StructField('codigo_justificativa', StringType()),\n",
    "        StructField('codigo_tipo_linha', StringType()),\n",
    "        StructField('icao_aerodromo_destino', StringType()),\n",
    "        StructField('icao_aerodromo_origem', StringType()),\n",
    "        StructField('icao_empresa_aerea', StringType()),\n",
    "        StructField('numero_voo', IntegerType()),\n",
    "        StructField('partida_prevista', IntegerType()),\n",
    "        StructField('partida_real', IntegerType()),\n",
    "        StructField('situacao_voo', StringType()),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df3 = (\n",
    "    spark\n",
    "    .read\n",
    "    .format(\"csv\")\n",
    "    .option(\"inferSchema\",\"true\")\n",
    "    .option(\"encoding\",\"utf-8\")\n",
    "    .option(\"delimiter\",\";\") #ou sep\n",
    "    .option(\"header\", \"true\")\n",
    "    .schema(schema_method)\n",
    "    .load(\"data/landing/VRA/\")\n",
    ")\n",
    "\n",
    "df3.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df3.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df3.show(2, truncate=False, vertical=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Entendendo as escritas"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

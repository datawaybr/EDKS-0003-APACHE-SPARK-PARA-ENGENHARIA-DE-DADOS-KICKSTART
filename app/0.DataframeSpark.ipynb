{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uso do Spark data Sources\n",
    "\n",
    "O **Spark Data Sources** é uma ferramenta poderosa no ecossistema Spark para carregar e salvar dados de várias fontes. Vamos dar uma olhada em como usá-lo!\n",
    "\n",
    "Abaixo os principais tipos de sources:\n",
    "\n",
    "| Tipo de Fonte de Dados | Descrição                                                                                     |\n",
    "|------------------------|-----------------------------------------------------------------------------------------------|\n",
    "| Parquet                | Formato de arquivo colunar eficiente e otimizado para análise que é amplamente suportado.    |\n",
    "| ORC                    | Formato de arquivo colunar otimizado para consulta que é amplamente usado em ecossistemas do Hadoop. |\n",
    "| JSON                   | Um formato de arquivo de dados textual amplamente utilizado.                                 |\n",
    "| CSV                    | Formato de arquivo de texto separado por vírgulas.                                            |\n",
    "| Avro                   | Estrutura de dados em formato binário compacto.                                                |\n",
    "| Delta Lake             | Uma camada de armazenamento de dados do tipo \"lakehouse\" que oferece controle transacional, governança e otimizações. |\n",
    "| Hadoop File System (HDFS) | Sistema de arquivos distribuído amplamente utilizado para armazenamento de dados.         |\n",
    "| Cassandra              | Banco de dados distribuído NoSQL amplamente utilizado.                                       |\n",
    "| Elasticsearch          | Motor de busca e análise em tempo real.                                                      |\n",
    "| Kafka                  | Plataforma de streaming distribuída para processamento de eventos em tempo real.             |\n",
    "| JDBC                   | Acesso a bancos de dados relacionais usando o Java Database Connectivity (JDBC).           |\n",
    "| Table                 | Consulta de tabelas usando SQL.                                                                |\n",
    "\n",
    "---\n",
    "## DataFrames no Apache Spark\n",
    "\n",
    "O Apache Spark é uma ferramenta poderosa para processamento de dados em larga escala e os DataFrames são uma parte fundamental dessa tecnologia. Um DataFrame é uma abstração de dados distribuídos que fornece uma interface fácil de usar para manipular dados em lote e em tempo real.\n",
    "\n",
    "## Características Principais\n",
    "\n",
    "Os DataFrames no Spark têm várias características essenciais:\n",
    "\n",
    "- **Estrutura de Dados**: Um DataFrame é uma estrutura de dados tabular que se assemelha a uma tabela em um banco de dados relacional. Ele tem linhas e colunas, permitindo que você organize e manipule dados de maneira eficiente.\n",
    "\n",
    "- **Tipagem de Colunas**: Cada coluna em um DataFrame possui um tipo de dados associado, permitindo que o Spark otimize o processamento e a execução de consultas.\n",
    "\n",
    "- **Transformações e Ações**: Você pode realizar transformações complexas nos DataFrames, como filtragem, agregação e junção de dados. Além disso, você pode executar ações para coletar resultados ou gravar dados em fontes externas.\n",
    "\n",
    "- **Integração com Diversas Fontes de Dados**: Os DataFrames no Spark podem ser usados com várias fontes de dados, como arquivos CSV, JSON, Parquet, bancos de dados relacionais, sistemas de mensagens, entre outros.\n",
    "\n",
    "- **Suporte a SQL**: Você pode executar consultas SQL em DataFrames, tornando-o uma ferramenta poderosa para análise de dados.\n",
    "\n",
    "## Carregando e Salvando Dados como dataframes\n",
    "\n",
    "Documentação oficial:\n",
    "- [Spark SQL Data Sources](https://spark.apache.org/docs/3.3.2/sql-data-sources.html)\n",
    "- [Spark DataFrames](https://spark.apache.org/docs/3.3.2/api/python/reference/pyspark.sql/dataframe.html)\n",
    "\n",
    "Aqui, vamos mostrar um exemplo de como carregar e salvar dados usando o Spark SQL Data Sources.\n",
    "\n",
    "### Carregando Dados\n",
    "\n",
    "Para carregar dados, você pode usar o método `read` do objeto `SparkSession`. Veja como:\n",
    "\n",
    "```python\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Exemplo\").getOrCreate()\n",
    "\n",
    "# Carregando dados de um arquivo CSV\n",
    "df = spark.read.csv(\"caminho/para/seu/arquivo.csv\")\n",
    "```\n",
    "\n",
    "A tabela abaixo detalha algumas opções comuns que você pode passar para o método read:\n",
    "\n",
    "| Opção            | Descrição                                    |\n",
    "|------------------|----------------------------------------------|\n",
    "| `format`         | Formato do arquivo (por ex., \"csv\", \"parquet\") |\n",
    "| `option`         | Opções específicas do formato                |\n",
    "| `header`         | Se a primeira linha contém nomes de colunas |\n",
    "| `inferSchema`    | Inferir automaticamente o esquema dos dados  |\n",
    "\n",
    "\n",
    "### Salvando Dados\n",
    "\n",
    "Agora, vamos ver como salvar dados usando o método `write`:\n",
    "\n",
    "```python\n",
    "# Salvando o DataFrame em formato Parquet\n",
    "df.write.parquet(\"caminho/para/seu/arquivo.parquet\")\n",
    "```\n",
    "\n",
    "Aqui estão algumas opções que você pode usar ao salvar:\n",
    "\n",
    "| Opção            | Descrição                                    |\n",
    "|------------------|----------------------------------------------|\n",
    "| `format`         | Formato de saída (por ex., \"parquet\", \"csv\") |\n",
    "| `mode`           | Modo de escrita (\"append\", \"overwrite\", etc.) |\n",
    "| `partitionBy`    | Colunas para particionar os dados           |\n",
    "| `option`         | Opções específicas do formato                |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "# Entendendo a leitura de dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": [],
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    " from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Exemplo\").config(\"spark.jars.packages\", \"org.postgresql:postgresql:42.2.24\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Entendendo o acesso a banco de dados com spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+--------------------+--------------------+------------+-----------+------------+--------------------+\n",
      "| id|customer_id|          ano_modelo|              modelo|  fabricante|ano_veiculo|   categoria|           dt_update|\n",
      "+---+-----------+--------------------+--------------------+------------+-----------+------------+--------------------+\n",
      "|  1|       9945|1996 Chevrolet Su...|       Toyota Previa|      Toyota|       2006| Van/Minivan|2023-11-09 00:30:...|\n",
      "|  2|       7080|2010 Ford F350 Su...|            Acura TL|  Land Rover|       2004|         SUV|2023-11-09 00:30:...|\n",
      "|  3|       1569|2009 Chrysler Seb...|Dodge Ram 2500 Me...|     Maybach|       2011| Convertible|2023-11-09 00:30:...|\n",
      "|  4|       3437|2012 Subaru Forester|   Ford C-MAX Hybrid|       Lexus|       2005|Wagon, Sedan|2023-11-09 00:30:...|\n",
      "|  5|       7159|2008 Chevrolet Su...|        Nissan Quest|  Oldsmobile|       2008|   Hatchback|2023-11-09 00:30:...|\n",
      "|  6|       1827|       2020 Lexus RC|       Toyota Avalon|     Mercury|       2018|Sedan, Wagon|2023-11-09 00:30:...|\n",
      "|  7|       6136|  2020 Jeep Renegade|Ford Ranger Super...|       Buick|       2005|         SUV|2023-11-09 00:30:...|\n",
      "|  8|       4218|2001 Ford Explore...|      Hyundai Accent|      Nissan|       2019|      Pickup|2023-11-09 00:30:...|\n",
      "|  9|       3101|1993 Oldsmobile B...|         Genesis G70|       Isuzu|       2018|       Coupe|2023-11-09 00:30:...|\n",
      "| 10|       9245|2003 Oldsmobile A...|           Acura TLX|       Lexus|       2016|         SUV|2023-11-09 00:30:...|\n",
      "| 11|       5445|        2015 Audi A8|         Ford Fusion|      Toyota|       2020|Wagon, Sedan|2023-11-09 00:30:...|\n",
      "| 12|       6520|   2007 Toyota Yaris|      Toyota Sequoia|Aston Martin|       1993|         SUV|2023-11-09 00:30:...|\n",
      "| 13|        723|2004 Ford F150 Su...|Chevrolet 3500 Re...|        Ford|       2012|         SUV|2023-11-09 00:30:...|\n",
      "| 14|       8191|   2000 BMW 7 Series|Chevrolet Silvera...|        Audi|       2013|Sedan, Wagon|2023-11-09 00:30:...|\n",
      "| 15|       8647|    2012 Ford Fiesta|      Oldsmobile LSS|        Ford|       2003|       Wagon|2023-11-09 00:30:...|\n",
      "| 16|       5070|1993 Mitsubishi P...|   Mitsubishi Mirage|   Chevrolet|       2003|         SUV|2023-11-09 00:30:...|\n",
      "| 17|       9794| 2005 Land Rover LR3|Mercedes-Benz Mer...|       Dodge|       2020|       Sedan|2023-11-09 00:30:...|\n",
      "| 18|       2550|     2012 GMC Acadia|Chevrolet Express...|     Hyundai|       2012|         SUV|2023-11-09 00:30:...|\n",
      "| 19|       6927|   2020 Nissan NV200|     McLaren MP4-12C|      Toyota|       1995|         SUV|2023-11-09 00:30:...|\n",
      "| 20|       8855|2018 Cadillac Esc...|Ford Econoline E1...|         GMC|       2005|      Pickup|2023-11-09 00:30:...|\n",
      "+---+-----------+--------------------+--------------------+------------+-----------+------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Leia dados do banco de dados usando o formato \"jdbc\"\n",
    "df = (\n",
    "    spark.read\n",
    "    .format(\"jdbc\")\n",
    "    .option(\"driver\", \"org.postgresql.Driver\")\n",
    "    .option(\"url\", \"jdbc:postgresql://postgres:5432/data-way-poc?user=admin-dataway&password=IloveDataway\")\n",
    "    .option(\"dbtable\", \"public.vehicle\")\n",
    "    #.option(\"query\", \"select * from public.vehicle\")\n",
    "    .option(\"user\", \"admin-dataway\")\n",
    "    .option(\"password\", \"IloveDataway\")\n",
    "    .load()\n",
    ")\n",
    "\n",
    "# Exiba os dados lidos do banco de dados\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Entendendo a leitura de arquivos com spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(_c0='ICAO Empresa Aerea;Numero Voo;Codigo DI;Codigo Tipo Linha;ICAO Aerodromo Origem;ICAO Aerodromo Destino;Partida Prevista;Partida Real;Chegada Prevista;Chegada Real;Situacao Voo;Codigo Justificativa')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Carregando dados de um arquivo CSV\n",
    "df = (\n",
    "    spark\n",
    "    .read\n",
    "    .csv(\"data/landing/VRA/\")\n",
    ")\n",
    "\n",
    "df.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|_c0                                                                                                                                                                                                 |\n",
      "+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|ICAO Empresa Aerea;Numero Voo;Codigo DI;Codigo Tipo Linha;ICAO Aerodromo Origem;ICAO Aerodromo Destino;Partida Prevista;Partida Real;Chegada Prevista;Chegada Real;Situacao Voo;Codigo Justificativa|\n",
      "+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(1, truncate= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- _c1: string (nullable = true)\n",
      " |-- _c2: string (nullable = true)\n",
      " |-- _c3: string (nullable = true)\n",
      " |-- _c4: string (nullable = true)\n",
      " |-- _c5: string (nullable = true)\n",
      " |-- _c6: string (nullable = true)\n",
      " |-- _c7: string (nullable = true)\n",
      " |-- _c8: string (nullable = true)\n",
      " |-- _c9: string (nullable = true)\n",
      " |-- _c10: string (nullable = true)\n",
      " |-- _c11: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2 = (\n",
    "    spark\n",
    "    .read\n",
    "    .format(\"csv\")\n",
    "    .option(\"inferSchema\",\"true\")\n",
    "    .option(\"encoding\",\"utf-8\")\n",
    "    .option(\"delimiter\",\";\") #ou sep\n",
    "    .load(\"data/landing/VRA/\")\n",
    ")\n",
    "\n",
    "df2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ICAO Empresa Aerea: string (nullable = true)\n",
      " |-- Numero Voo: string (nullable = true)\n",
      " |-- Codigo DI: string (nullable = true)\n",
      " |-- Codigo Tipo Linha: string (nullable = true)\n",
      " |-- ICAO Aerodromo Origem: string (nullable = true)\n",
      " |-- ICAO Aerodromo Destino: string (nullable = true)\n",
      " |-- Partida Prevista: string (nullable = true)\n",
      " |-- Partida Real: string (nullable = true)\n",
      " |-- Chegada Prevista: string (nullable = true)\n",
      " |-- Chegada Real: string (nullable = true)\n",
      " |-- Situacao Voo: string (nullable = true)\n",
      " |-- Codigo Justificativa: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3 = (\n",
    "    spark\n",
    "    .read\n",
    "    .format(\"csv\")\n",
    "    .option(\"inferSchema\",\"true\")\n",
    "    .option(\"encoding\",\"utf-8\")\n",
    "    .option(\"delimiter\",\";\") #ou sep\n",
    "    .option(\"header\", \"true\")\n",
    "    .load(\"data/landing/VRA/\")\n",
    ")\n",
    "\n",
    "df3.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2709899"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Entendendo as escritas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "(\n",
    "    df3\n",
    "    .write\n",
    "    .format(\"orc\")\n",
    "    #.partitionBy(\"Codigo Justificativa\")\n",
    "    .mode(\"append\")\n",
    "    .save(\"/home/app/data/landing/vra_orc/\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "(\n",
    "    df3\n",
    "    .write\n",
    "    .format(\"parquet\")\n",
    "    .partitionBy(\"Codigo Justificativa\")\n",
    "    .mode(\"overwrite\")\n",
    "    .save(\"/home/app/data/1.bronze/vra_test/\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Entendendo o acesso a banco de dados com spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "# Entendendo os schemas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('ICAO Empresa Aerea', StringType(), True), StructField('Numero Voo', StringType(), True), StructField('Codigo DI', StringType(), True), StructField('Codigo Tipo Linha', StringType(), True), StructField('ICAO Aerodromo Origem', StringType(), True), StructField('ICAO Aerodromo Destino', StringType(), True), StructField('Partida Prevista', StringType(), True), StructField('Partida Real', StringType(), True), StructField('Chegada Prevista', StringType(), True), StructField('Chegada Real', StringType(), True), StructField('Situacao Voo', StringType(), True), StructField('Codigo Justificativa', StringType(), True)])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'`ICAO Empresa Aerea` STRING,`Numero Voo` STRING,`Codigo DI` STRING,`Codigo Tipo Linha` STRING,`ICAO Aerodromo Origem` STRING,`ICAO Aerodromo Destino` STRING,`Partida Prevista` STRING,`Partida Real` STRING,`Chegada Prevista` STRING,`Chegada Real` STRING,`Situacao Voo` STRING,`Codigo Justificativa` STRING'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "schema_json = df3.schema.json()\n",
    "ddl = spark.sparkContext._jvm.org.apache.spark.sql.types.DataType.fromJson(schema_json).toDDL()\n",
    "\n",
    "ddl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "schema_ddl = \"\"\"\n",
    "    `ICAO Empresa Aerea` STRING,\n",
    "    `Numero Voo` STRING,\n",
    "    `Codigo DI` STRING,\n",
    "    `Codigo Tipo Linha` STRING,\n",
    "    `ICAO Aerodromo Origem` STRING,\n",
    "    `ICAO Aerodromo Destino` STRING,\n",
    "    `Partida Prevista` timestamp,\n",
    "    `Partida Real` timestamp,\n",
    "    `Chegada Prevista` timestamp,\n",
    "    `Chegada Real` timestamp,\n",
    "    `Situacao Voo` STRING,\n",
    "    `Codigo Justificativa` STRING\n",
    "\"\"\"\n",
    "\n",
    "schema_method = StructType(\n",
    "    [\n",
    "        StructField('chegada_prevista', IntegerType()),\n",
    "        StructField('chegada_real', IntegerType()),\n",
    "        StructField('codigo_autorizacao', StringType()),\n",
    "        StructField('codigo_justificativa', StringType()),\n",
    "        StructField('codigo_tipo_linha', StringType()),\n",
    "        StructField('icao_aerodromo_destino', StringType()),\n",
    "        StructField('icao_aerodromo_origem', StringType()),\n",
    "        StructField('icao_empresa_aerea', StringType()),\n",
    "        StructField('numero_voo', IntegerType()),\n",
    "        StructField('partida_prevista', IntegerType()),\n",
    "        StructField('partida_real', IntegerType()),\n",
    "        StructField('situacao_voo', StringType()),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ICAO Empresa Aerea: string (nullable = true)\n",
      " |-- Numero Voo: string (nullable = true)\n",
      " |-- Codigo DI: string (nullable = true)\n",
      " |-- Codigo Tipo Linha: string (nullable = true)\n",
      " |-- ICAO Aerodromo Origem: string (nullable = true)\n",
      " |-- ICAO Aerodromo Destino: string (nullable = true)\n",
      " |-- Partida Prevista: string (nullable = true)\n",
      " |-- Partida Real: string (nullable = true)\n",
      " |-- Chegada Prevista: string (nullable = true)\n",
      " |-- Chegada Real: string (nullable = true)\n",
      " |-- Situacao Voo: string (nullable = true)\n",
      " |-- Codigo Justificativa: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df4 = (\n",
    "    spark\n",
    "    .read\n",
    "    .format(\"csv\")\n",
    "    #.option(\"inferSchema\",\"true\")\n",
    "    .option(\"encoding\",\"utf-8\")\n",
    "    .option(\"delimiter\",\";\") #ou sep\n",
    "    .option(\"header\", \"true\")\n",
    "    .schema(schema_ddl)\n",
    "    .load(\"data/landing/VRA/\")\n",
    ")\n",
    "\n",
    "df3.printSchema()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

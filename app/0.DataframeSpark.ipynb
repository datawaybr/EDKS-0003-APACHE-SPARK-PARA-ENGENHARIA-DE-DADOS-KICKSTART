{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uso do Spark data Sources\n",
    "\n",
    "O **Spark Data Sources** é uma ferramenta poderosa no ecossistema Spark para carregar e salvar dados de várias fontes. Vamos dar uma olhada em como usá-lo!\n",
    "\n",
    "Abaixo os principais tipos de sources:\n",
    "\n",
    "| Tipo de Fonte de Dados | Descrição                                                                                     |\n",
    "|------------------------|-----------------------------------------------------------------------------------------------|\n",
    "| Parquet                | Formato de arquivo colunar eficiente e otimizado para análise que é amplamente suportado.    |\n",
    "| ORC                    | Formato de arquivo colunar otimizado para consulta que é amplamente usado em ecossistemas do Hadoop. |\n",
    "| JSON                   | Um formato de arquivo de dados textual amplamente utilizado.                                 |\n",
    "| CSV                    | Formato de arquivo de texto separado por vírgulas.                                            |\n",
    "| Avro                   | Estrutura de dados em formato binário compacto.                                                |\n",
    "| Delta Lake             | Uma camada de armazenamento de dados do tipo \"lakehouse\" que oferece controle transacional, governança e otimizações. |\n",
    "| Hadoop File System (HDFS) | Sistema de arquivos distribuído amplamente utilizado para armazenamento de dados.         |\n",
    "| Cassandra              | Banco de dados distribuído NoSQL amplamente utilizado.                                       |\n",
    "| Elasticsearch          | Motor de busca e análise em tempo real.                                                      |\n",
    "| Kafka                  | Plataforma de streaming distribuída para processamento de eventos em tempo real.             |\n",
    "| JDBC                   | Acesso a bancos de dados relacionais usando o Java Database Connectivity (JDBC).           |\n",
    "| Table                 | Consulta de tabelas usando SQL.                                                                |\n",
    "\n",
    "---\n",
    "## DataFrames no Apache Spark\n",
    "\n",
    "O Apache Spark é uma ferramenta poderosa para processamento de dados em larga escala e os DataFrames são uma parte fundamental dessa tecnologia. Um DataFrame é uma abstração de dados distribuídos que fornece uma interface fácil de usar para manipular dados em lote e em tempo real.\n",
    "\n",
    "## Características Principais\n",
    "\n",
    "Os DataFrames no Spark têm várias características essenciais:\n",
    "\n",
    "- **Estrutura de Dados**: Um DataFrame é uma estrutura de dados tabular que se assemelha a uma tabela em um banco de dados relacional. Ele tem linhas e colunas, permitindo que você organize e manipule dados de maneira eficiente.\n",
    "\n",
    "- **Tipagem de Colunas**: Cada coluna em um DataFrame possui um tipo de dados associado, permitindo que o Spark otimize o processamento e a execução de consultas.\n",
    "\n",
    "- **Transformações e Ações**: Você pode realizar transformações complexas nos DataFrames, como filtragem, agregação e junção de dados. Além disso, você pode executar ações para coletar resultados ou gravar dados em fontes externas.\n",
    "\n",
    "- **Integração com Diversas Fontes de Dados**: Os DataFrames no Spark podem ser usados com várias fontes de dados, como arquivos CSV, JSON, Parquet, bancos de dados relacionais, sistemas de mensagens, entre outros.\n",
    "\n",
    "- **Suporte a SQL**: Você pode executar consultas SQL em DataFrames, tornando-o uma ferramenta poderosa para análise de dados.\n",
    "\n",
    "## Carregando e Salvando Dados como dataframes\n",
    "\n",
    "Documentação oficial:\n",
    "- [Spark SQL Data Sources](https://spark.apache.org/docs/3.3.2/sql-data-sources.html)\n",
    "- [Spark DataFrames](https://spark.apache.org/docs/3.3.2/api/python/reference/pyspark.sql/dataframe.html)\n",
    "\n",
    "Aqui, vamos mostrar um exemplo de como carregar e salvar dados usando o Spark SQL Data Sources.\n",
    "\n",
    "### Carregando Dados\n",
    "\n",
    "Para carregar dados, você pode usar o método `read` do objeto `SparkSession`. Veja como:\n",
    "\n",
    "```python\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Exemplo\").getOrCreate()\n",
    "\n",
    "# Carregando dados de um arquivo CSV\n",
    "df = spark.read.csv(\"caminho/para/seu/arquivo.csv\")\n",
    "```\n",
    "\n",
    "A tabela abaixo detalha algumas opções comuns que você pode passar para o método read:\n",
    "\n",
    "| Opção            | Descrição                                    |\n",
    "|------------------|----------------------------------------------|\n",
    "| `format`         | Formato do arquivo (por ex., \"csv\", \"parquet\") |\n",
    "| `option`         | Opções específicas do formato                |\n",
    "| `header`         | Se a primeira linha contém nomes de colunas |\n",
    "| `inferSchema`    | Inferir automaticamente o esquema dos dados  |\n",
    "\n",
    "\n",
    "### Salvando Dados\n",
    "\n",
    "Agora, vamos ver como salvar dados usando o método `write`:\n",
    "\n",
    "```python\n",
    "# Salvando o DataFrame em formato Parquet\n",
    "df.write.parquet(\"caminho/para/seu/arquivo.parquet\")\n",
    "```\n",
    "\n",
    "Aqui estão algumas opções que você pode usar ao salvar:\n",
    "\n",
    "| Opção            | Descrição                                    |\n",
    "|------------------|----------------------------------------------|\n",
    "| `format`         | Formato de saída (por ex., \"parquet\", \"csv\") |\n",
    "| `mode`           | Modo de escrita (\"append\", \"overwrite\", etc.) |\n",
    "| `partitionBy`    | Colunas para particionar os dados           |\n",
    "| `option`         | Opções específicas do formato                |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Entendendo a leitura de dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [],
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    " from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Exemplo\").config(\"spark.jars.packages\", \"org.postgresql:postgresql:42.2.24\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Carregando dados de um arquivo CSV\n",
    "df = (\n",
    "    spark\n",
    "    .read\n",
    "    .csv(\"data/landing/VRA/\")\n",
    ")\n",
    "df.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df2 = (\n",
    "    spark\n",
    "    .read\n",
    "    .format(\"csv\")\n",
    "    .option(\"inferSchema\",\"true\")\n",
    "    .option(\"encoding\",\"utf-8\")\n",
    "    .option(\"delimiter\",\";\") #ou sep\n",
    "    .load(\"data/landing/VRA/\")\n",
    ")\n",
    "\n",
    "df2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df3 = (\n",
    "    spark\n",
    "    .read\n",
    "    .format(\"csv\")\n",
    "    .option(\"inferSchema\",\"true\")\n",
    "    .option(\"encoding\",\"utf-8\")\n",
    "    .option(\"delimiter\",\";\") #ou sep\n",
    "    .option(\"header\", \"true\")\n",
    "    .load(\"data/landing/VRA/\")\n",
    ")\n",
    "\n",
    "df3.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Entendendo os schemas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "schema_json = df3.schema.json()\n",
    "ddl = spark.sparkContext._jvm.org.apache.spark.sql.types.DataType.fromJson(schema_json).toDDL()\n",
    "\n",
    "ddl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "schema_ddl = \"\"\"\n",
    "    `ICAO Empresa Aerea` STRING,\n",
    "    `Numero Voo` STRING,\n",
    "    `Codigo DI` STRING,\n",
    "    `Codigo Tipo Linha` STRING,\n",
    "    `ICAO Aerodromo Origem` STRING,\n",
    "    `ICAO Aerodromo Destino` STRING,\n",
    "    `Partida Prevista` timestamp,\n",
    "    `Partida Real` timestamp,\n",
    "    `Chegada Prevista` timestamp,\n",
    "    `Chegada Real` timestamp,\n",
    "    `Situacao Voo` STRING,\n",
    "    `Codigo Justificativa` STRING\n",
    "\"\"\"\n",
    "\n",
    "schema_method = StructType(\n",
    "    [\n",
    "        StructField('chegada_prevista', IntegerType()),\n",
    "        StructField('chegada_real', IntegerType()),\n",
    "        StructField('codigo_autorizacao', StringType()),\n",
    "        StructField('codigo_justificativa', StringType()),\n",
    "        StructField('codigo_tipo_linha', StringType()),\n",
    "        StructField('icao_aerodromo_destino', StringType()),\n",
    "        StructField('icao_aerodromo_origem', StringType()),\n",
    "        StructField('icao_empresa_aerea', StringType()),\n",
    "        StructField('numero_voo', IntegerType()),\n",
    "        StructField('partida_prevista', IntegerType()),\n",
    "        StructField('partida_real', IntegerType()),\n",
    "        StructField('situacao_voo', StringType()),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df4 = (\n",
    "    spark\n",
    "    .read\n",
    "    .format(\"csv\")\n",
    "    .option(\"inferSchema\",\"true\")\n",
    "    .option(\"encoding\",\"utf-8\")\n",
    "    .option(\"delimiter\",\";\") #ou sep\n",
    "    .option(\"header\", \"true\")\n",
    "    .schema(schema_method)\n",
    "    .load(\"data/landing/VRA/\")\n",
    ")\n",
    "\n",
    "df3.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df4.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df4.show(2, truncate=False, vertical=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Entendendo o acesso a banco de dados com spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+--------------------+--------------------+-----------+-----------+----------------+--------------------+\n",
      "| id|customer_id|          ano_modelo|              modelo| fabricante|ano_veiculo|       categoria|           dt_update|\n",
      "+---+-----------+--------------------+--------------------+-----------+-----------+----------------+--------------------+\n",
      "|  1|       1764|        2003 Audi S6|         Buick Regal|      smart|       2006|          Pickup|2023-11-03 12:54:...|\n",
      "|  2|       6344|  2003 Suzuki Vitara|        BMW 5 Series|    Bentley|       2012|          Pickup|2023-11-03 12:54:...|\n",
      "|  3|       9596|  2002 Toyota Avalon|GMC Sierra 1500 C...|     Subaru|       2010|     Van/Minivan|2023-11-03 12:54:...|\n",
      "|  4|       3749|2009 Chevrolet Si...|       Tesla Model 3|  Chevrolet|       2010|             SUV|2023-11-03 12:54:...|\n",
      "|  5|       8169|2000 Lincoln Cont...|Dodge Ram 3500 Re...|        BMW|       2009|     Van/Minivan|2023-11-03 12:54:...|\n",
      "|  6|       5499|1992 Mercedes-Ben...|    Aston Martin DB9|   Cadillac|       2018|Hatchback, Wagon|2023-11-03 12:54:...|\n",
      "|  7|       9340|2001 Dodge Ram 35...|     Hyundai Tiburon|      Scion|       2001|          Pickup|2023-11-03 12:54:...|\n",
      "|  8|       3335|      2000 Volvo S80|Mercedes-Benz Mer...|     Nissan|       2006|       Hatchback|2023-11-03 12:54:...|\n",
      "|  9|       1478|    1996 Dodge Viper|GMC Sierra 2500 H...|  Chevrolet|       2009|     Van/Minivan|2023-11-03 12:54:...|\n",
      "| 10|       7225| 1997 Audi Cabriolet|        Nissan Quest| Mitsubishi|       2015|          Pickup|2023-11-03 12:54:...|\n",
      "| 11|       1201|1995 Plymouth Acc...|           Acura SLX|     Toyota|       2003|           Coupe|2023-11-03 12:54:...|\n",
      "| 12|       3593|2000 Mercedes-Ben...|   Chevrolet Corsica|       Audi|       1994|           Sedan|2023-11-03 12:54:...|\n",
      "| 13|       3436|   1993 Eagle Vision|       Ford Explorer| Land Rover|       2003|          Pickup|2023-11-03 12:54:...|\n",
      "| 14|       6055|       2015 Lexus ES|           FIAT 500e|     Toyota|       1997|             SUV|2023-11-03 12:54:...|\n",
      "| 15|       8865|  1997 Buick Century|            Lexus HS|     Toyota|       1999|     Convertible|2023-11-03 12:54:...|\n",
      "| 16|       4391|2020 Hyundai Pali...|    Cadillac DeVille|Lamborghini|       2019|          Pickup|2023-11-03 12:54:...|\n",
      "| 17|       7769|      2013 Acura ZDX|Dodge Ram Wagon B350|       Ford|       2007|           Sedan|2023-11-03 12:54:...|\n",
      "| 18|       7514|2004 Chevrolet Ve...|Dodge Ram 1500 Cl...|    Hyundai|       2015|          Pickup|2023-11-03 12:54:...|\n",
      "| 19|       8471|        2014 Audi S6|           Acura MDX|     Toyota|       2003|       Hatchback|2023-11-03 12:54:...|\n",
      "| 20|       8993|  2010 Toyota Matrix|Chevrolet Suburba...|     Jaguar|       2002|             SUV|2023-11-03 12:54:...|\n",
      "+---+-----------+--------------------+--------------------+-----------+-----------+----------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Leia dados do banco de dados usando o formato \"jdbc\"\n",
    "df = (\n",
    "    spark.read\n",
    "    .format(\"jdbc\")\n",
    "    .option(\"driver\", \"org.postgresql.Driver\")\n",
    "    .option(\"url\", \"jdbc:postgresql://postgres:5432/data-way-poc?user=admin-dataway&password=IloveDataway\") \n",
    "    .option(\"dbtable\", \"public.vehicle\")\n",
    "    .option(\"user\", \"admin-dataway\")\n",
    "    .option(\"password\", \"IloveDataway\")\n",
    "    .load()\n",
    ")\n",
    "\n",
    "# Exiba os dados lidos do banco de dados\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Entendendo as escritas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "(\n",
    "    df3\n",
    "    .write\n",
    "    .format(\"parquet\")\n",
    "    #.partitionBY()\n",
    "    .mode(\"append\")\n",
    "    .save(\"/home/app/data/1.bronze\")\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

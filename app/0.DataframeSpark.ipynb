{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uso do Spark data Sources\n",
    "\n",
    "O **Spark Data Sources** é uma ferramenta poderosa no ecossistema Spark para carregar e salvar dados de várias fontes. Vamos dar uma olhada em como usá-lo!\n",
    "\n",
    "Abaixo os principais tipos de sources:\n",
    "\n",
    "| Tipo de Fonte de Dados | Descrição                                                                                     |\n",
    "|------------------------|-----------------------------------------------------------------------------------------------|\n",
    "| Parquet                | Formato de arquivo colunar eficiente e otimizado para análise que é amplamente suportado.    |\n",
    "| ORC                    | Formato de arquivo colunar otimizado para consulta que é amplamente usado em ecossistemas do Hadoop. |\n",
    "| JSON                   | Um formato de arquivo de dados textual amplamente utilizado.                                 |\n",
    "| CSV                    | Formato de arquivo de texto separado por vírgulas.                                            |\n",
    "| Avro                   | Estrutura de dados em formato binário compacto.                                                |\n",
    "| Delta Lake             | Uma camada de armazenamento de dados do tipo \"lakehouse\" que oferece controle transacional, governança e otimizações. |\n",
    "| Hadoop File System (HDFS) | Sistema de arquivos distribuído amplamente utilizado para armazenamento de dados.         |\n",
    "| Cassandra              | Banco de dados distribuído NoSQL amplamente utilizado.                                       |\n",
    "| Elasticsearch          | Motor de busca e análise em tempo real.                                                      |\n",
    "| Kafka                  | Plataforma de streaming distribuída para processamento de eventos em tempo real.             |\n",
    "| JDBC                   | Acesso a bancos de dados relacionais usando o Java Database Connectivity (JDBC).           |\n",
    "| Table                 | Consulta de tabelas usando SQL.                                                                |\n",
    "\n",
    "---\n",
    "## DataFrames no Apache Spark\n",
    "\n",
    "O Apache Spark é uma ferramenta poderosa para processamento de dados em larga escala e os DataFrames são uma parte fundamental dessa tecnologia. Um DataFrame é uma abstração de dados distribuídos que fornece uma interface fácil de usar para manipular dados em lote e em tempo real.\n",
    "\n",
    "## Características Principais\n",
    "\n",
    "Os DataFrames no Spark têm várias características essenciais:\n",
    "\n",
    "- **Estrutura de Dados**: Um DataFrame é uma estrutura de dados tabular que se assemelha a uma tabela em um banco de dados relacional. Ele tem linhas e colunas, permitindo que você organize e manipule dados de maneira eficiente.\n",
    "\n",
    "- **Tipagem de Colunas**: Cada coluna em um DataFrame possui um tipo de dados associado, permitindo que o Spark otimize o processamento e a execução de consultas.\n",
    "\n",
    "- **Transformações e Ações**: Você pode realizar transformações complexas nos DataFrames, como filtragem, agregação e junção de dados. Além disso, você pode executar ações para coletar resultados ou gravar dados em fontes externas.\n",
    "\n",
    "- **Integração com Diversas Fontes de Dados**: Os DataFrames no Spark podem ser usados com várias fontes de dados, como arquivos CSV, JSON, Parquet, bancos de dados relacionais, sistemas de mensagens, entre outros.\n",
    "\n",
    "- **Suporte a SQL**: Você pode executar consultas SQL em DataFrames, tornando-o uma ferramenta poderosa para análise de dados.\n",
    "\n",
    "## Carregando e Salvando Dados como dataframes\n",
    "\n",
    "Documentação oficial:\n",
    "- [Spark SQL Data Sources](https://spark.apache.org/docs/3.3.2/sql-data-sources.html)\n",
    "- [Spark DataFrames](https://spark.apache.org/docs/3.3.2/api/python/reference/pyspark.sql/dataframe.html)\n",
    "\n",
    "Aqui, vamos mostrar um exemplo de como carregar e salvar dados usando o Spark SQL Data Sources.\n",
    "\n",
    "### Carregando Dados\n",
    "\n",
    "Para carregar dados, você pode usar o método `read` do objeto `SparkSession`. Veja como:\n",
    "\n",
    "```python\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Exemplo\").getOrCreate()\n",
    "\n",
    "# Carregando dados de um arquivo CSV\n",
    "df = spark.read.csv(\"caminho/para/seu/arquivo.csv\")\n",
    "```\n",
    "\n",
    "A tabela abaixo detalha algumas opções comuns que você pode passar para o método read:\n",
    "\n",
    "| Opção            | Descrição                                    |\n",
    "|------------------|----------------------------------------------|\n",
    "| `format`         | Formato do arquivo (por ex., \"csv\", \"parquet\") |\n",
    "| `option`         | Opções específicas do formato                |\n",
    "| `header`         | Se a primeira linha contém nomes de colunas |\n",
    "| `inferSchema`    | Inferir automaticamente o esquema dos dados  |\n",
    "\n",
    "\n",
    "### Salvando Dados\n",
    "\n",
    "Agora, vamos ver como salvar dados usando o método `write`:\n",
    "\n",
    "```python\n",
    "# Salvando o DataFrame em formato Parquet\n",
    "df.write.parquet(\"caminho/para/seu/arquivo.parquet\")\n",
    "```\n",
    "\n",
    "Aqui estão algumas opções que você pode usar ao salvar:\n",
    "\n",
    "| Opção            | Descrição                                    |\n",
    "|------------------|----------------------------------------------|\n",
    "| `format`         | Formato de saída (por ex., \"parquet\", \"csv\") |\n",
    "| `mode`           | Modo de escrita (\"append\", \"overwrite\", etc.) |\n",
    "| `partitionBy`    | Colunas para particionar os dados           |\n",
    "| `option`         | Opções específicas do formato                |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
